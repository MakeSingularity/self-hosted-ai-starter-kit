{
  "meta": {
    "instanceId": "enhanced-ai-monitoring-workflow"
  },
  "name": "Enhanced AI System Monitor & Oliver Integration",
  "nodes": [
    {
      "parameters": {},
      "id": "manual-trigger",
      "name": "Manual Trigger",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [200, 300]
    },
    {
      "parameters": {
        "triggerTimes": {
          "item": [
            {
              "hour": 0,
              "minute": 0
            },
            {
              "hour": 6,
              "minute": 0
            },
            {
              "hour": 12,
              "minute": 0
            },
            {
              "hour": 18,
              "minute": 0
            }
          ]
        }
      },
      "id": "schedule-trigger",
      "name": "Scheduled Health Check",
      "type": "n8n-nodes-base.cron",
      "typeVersion": 1,
      "position": [200, 200]
    },
    {
      "parameters": {
        "path": "health-check",
        "httpMethod": "GET",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "webhook-trigger",
      "name": "Health Check API",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [200, 400],
      "webhookId": "health-check-webhook"
    },
    {
      "parameters": {
        "command": "python /app/shared/system_metrics.py",
        "options": {}
      },
      "id": "system-metrics",
      "name": "Get System Metrics",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [420, 300]
    },
    {
      "parameters": {
        "url": "http://host.docker.internal:5432",
        "options": {
          "timeout": 5000
        }
      },
      "id": "postgres-health",
      "name": "Check PostgreSQL",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [640, 200]
    },
    {
      "parameters": {
        "url": "http://host.docker.internal:11434/api/tags",
        "options": {
          "timeout": 10000
        }
      },
      "id": "ollama-health",
      "name": "Check Ollama Models",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [640, 300]
    },
    {
      "parameters": {
        "url": "http://host.docker.internal:6333/collections",
        "options": {
          "timeout": 10000
        }
      },
      "id": "qdrant-health",
      "name": "Check Qdrant",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [640, 400]
    },
    {
      "parameters": {
        "url": "http://host.docker.internal:5678/api/v1/workflows",
        "options": {
          "timeout": 5000
        }
      },
      "id": "n8n-health",
      "name": "Check n8n API",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [640, 500]
    },
    {
      "parameters": {
        "command": "python /app/shared/tts_stt_checker.py",
        "options": {}
      },
      "id": "tts-stt-check",
      "name": "Check TTS/STT Readiness",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [420, 200]
    },
    {
      "parameters": {
        "command": "python -c \"import json; print(json.dumps({'container_info': 'Running inside n8n container', 'hostname': open('/etc/hostname').read().strip()}))\"",
        "options": {}
      },
      "id": "docker-services",
      "name": "Check Container Info",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [420, 400]
    },
    {
      "parameters": {
        "jsCode": "// Enhanced AI System Analysis with TTS/STT Readiness\nconst items = $input.all();\n\n// Parse system metrics\nlet systemMetrics = {};\ntry {\n  const metricsItem = items.find(item => item.json && typeof item.json === 'string' && item.json.includes('python_version'));\n  if (metricsItem) {\n    systemMetrics = JSON.parse(metricsItem.json);\n  }\n} catch (e) {\n  console.log('Could not parse system metrics');\n}\n\n// Parse GPU check\nlet gpuAvailable = false;\ntry {\n  const gpuItem = items.find(item => item.json && typeof item.json === 'string' && item.json.includes('GPU Available'));\n  if (gpuItem) {\n    gpuAvailable = gpuItem.json.includes('True');\n  }\n} catch (e) {\n  console.log('Could not parse GPU status');\n}\n\n// Check service health\nconst services = {\n  postgresql: {\n    status: items.find(item => item.json && !item.json.error && item.json.toString().includes('5432')) ? 'running' : 'stopped',\n    health: 'unknown',\n    required_for: ['data_storage', 'user_management']\n  },\n  ollama: {\n    status: items.find(item => item.json && item.json.models) ? 'running' : 'stopped',\n    models: items.find(item => item.json && item.json.models)?.json?.models?.length || 0,\n    health: items.find(item => item.json && item.json.models) ? 'healthy' : 'unhealthy',\n    required_for: ['llm_inference', 'text_generation', 'conversation']\n  },\n  qdrant: {\n    status: items.find(item => item.json && item.json.result) ? 'running' : 'stopped',\n    collections: items.find(item => item.json && item.json.result)?.json?.result?.collections?.length || 0,\n    health: items.find(item => item.json && item.json.result) ? 'healthy' : 'unhealthy',\n    required_for: ['vector_search', 'rag', 'semantic_search']\n  },\n  n8n: {\n    status: items.find(item => item.json && Array.isArray(item.json.data)) ? 'running' : 'stopped',\n    workflows: items.find(item => item.json && Array.isArray(item.json.data))?.json?.data?.length || 0,\n    health: items.find(item => item.json && Array.isArray(item.json.data)) ? 'healthy' : 'unhealthy',\n    required_for: ['automation', 'workflow_orchestration', 'oliver_brain']\n  }\n};\n\n// Enhanced TTS/STT Readiness Assessment\nconst ttsSTTReadiness = {\n  hardware_ready: {\n    cpu_sufficient: systemMetrics.cpu_percent < 80,\n    memory_sufficient: systemMetrics.memory_usage?.available > 2000000000, // 2GB available\n    gpu_available: gpuAvailable,\n    disk_space_sufficient: systemMetrics.disk_usage?.free > 5000000000 // 5GB free\n  },\n  services_ready: {\n    ollama_running: services.ollama.status === 'running',\n    ollama_has_models: services.ollama.models > 0,\n    n8n_running: services.n8n.status === 'running',\n    vector_db_ready: services.qdrant.status === 'running'\n  },\n  recommended_models: [\n    {\n      name: 'whisper-base',\n      purpose: 'Speech-to-Text',\n      status: 'not_checked',\n      size_mb: 142,\n      command: 'ollama pull whisper-base'\n    },\n    {\n      name: 'bark',\n      purpose: 'Text-to-Speech',\n      status: 'not_checked', \n      size_mb: 2800,\n      command: 'ollama pull bark'\n    },\n    {\n      name: 'llama3.2:latest',\n      purpose: 'Oliver LLM Brain',\n      status: services.ollama.models > 0 ? 'available' : 'missing',\n      size_mb: 2000,\n      command: 'ollama pull llama3.2:latest'\n    }\n  ],\n  integration_points: {\n    oliver_webhook_ready: services.n8n.status === 'running',\n    audio_processing_ready: gpuAvailable || systemMetrics.cpu_percent < 60,\n    real_time_processing: systemMetrics.memory_usage?.available > 4000000000 // 4GB for real-time\n  }\n};\n\n// Calculate overall readiness scores\nfunction calculateReadinessScore(category) {\n  const items = Object.values(category);\n  const ready = items.filter(item => item === true).length;\n  return Math.round((ready / items.length) * 100);\n}\n\nconst hardwareScore = calculateReadinessScore(ttsSTTReadiness.hardware_ready);\nconst servicesScore = calculateReadinessScore(ttsSTTReadiness.services_ready);\nconst integrationScore = calculateReadinessScore(ttsSTTReadiness.integration_points);\n\n// Oliver Integration Status\nconst oliverStatus = {\n  brain_ready: services.ollama.status === 'running' && services.ollama.models > 0,\n  automation_ready: services.n8n.status === 'running',\n  memory_ready: services.qdrant.status === 'running',\n  voice_interface_ready: hardwareScore >= 75 && servicesScore >= 75,\n  overall_readiness: Math.round((hardwareScore + servicesScore + integrationScore) / 3)\n};\n\n// Generate intelligent recommendations\nfunction generateRecommendations() {\n  const recommendations = [];\n  \n  // Service recommendations\n  Object.entries(services).forEach(([name, service]) => {\n    if (service.status !== 'running') {\n      recommendations.push({\n        priority: 'high',\n        category: 'service',\n        action: `Start ${name} service`,\n        command: name === 'postgresql' ? 'docker-compose up -d postgres' : \n                name === 'ollama' ? 'docker-compose up -d ollama' :\n                name === 'qdrant' ? 'docker-compose up -d qdrant' :\n                'docker-compose up -d',\n        impact: service.required_for.join(', ')\n      });\n    }\n  });\n  \n  // TTS/STT recommendations\n  if (hardwareScore < 75) {\n    recommendations.push({\n      priority: 'medium',\n      category: 'hardware',\n      action: 'Consider hardware upgrades for optimal TTS/STT performance',\n      details: `GPU: ${gpuAvailable ? 'Available' : 'Not Available'}, Memory: ${Math.round(systemMetrics.memory_usage?.available / 1024 / 1024 / 1024)}GB available`\n    });\n  }\n  \n  // Model recommendations\n  ttsSTTReadiness.recommended_models.forEach(model => {\n    if (model.status === 'missing') {\n      recommendations.push({\n        priority: 'medium',\n        category: 'model',\n        action: `Install ${model.name} model for ${model.purpose}`,\n        command: model.command,\n        size: `${model.size_mb}MB`\n      });\n    }\n  });\n  \n  // Oliver-specific recommendations\n  if (!oliverStatus.brain_ready) {\n    recommendations.push({\n      priority: 'high',\n      category: 'oliver',\n      action: 'Prepare Oliver AI Brain',\n      details: 'Install LLM models and ensure Ollama is running for Oliver conversations'\n    });\n  }\n  \n  if (!oliverStatus.voice_interface_ready) {\n    recommendations.push({\n      priority: 'medium',\n      category: 'oliver',\n      action: 'Prepare Oliver Voice Interface',\n      details: 'Install TTS/STT models and optimize hardware for real-time voice processing'\n    });\n  }\n  \n  return recommendations;\n}\n\n// Generate the comprehensive status report\nconst enhancedReport = {\n  timestamp: new Date().toISOString(),\n  system: {\n    python_version: systemMetrics.python_version?.split(' ')[0] || 'unknown',\n    cpu_usage: systemMetrics.cpu_percent || 0,\n    memory_usage_gb: Math.round(systemMetrics.memory_usage?.used / 1024 / 1024 / 1024) || 0,\n    memory_available_gb: Math.round(systemMetrics.memory_usage?.available / 1024 / 1024 / 1024) || 0,\n    disk_free_gb: Math.round(systemMetrics.disk_usage?.free / 1024 / 1024 / 1024) || 0,\n    gpu_available: gpuAvailable\n  },\n  services,\n  tts_stt_readiness: {\n    hardware_score: hardwareScore,\n    services_score: servicesScore,\n    integration_score: integrationScore,\n    overall_ready: hardwareScore >= 75 && servicesScore >= 75,\n    details: ttsSTTReadiness\n  },\n  oliver_status: oliverStatus,\n  recommendations: generateRecommendations(),\n  health_summary: {\n    total_services: Object.keys(services).length,\n    running_services: Object.values(services).filter(s => s.status === 'running').length,\n    healthy_services: Object.values(services).filter(s => s.health === 'healthy').length,\n    system_load: systemMetrics.cpu_percent < 80 ? 'normal' : 'high',\n    memory_pressure: systemMetrics.memory_usage?.percent > 85 ? 'high' : 'normal'\n  },\n  alerts: [\n    ...(systemMetrics.cpu_percent > 90 ? [{ level: 'critical', message: 'High CPU usage detected' }] : []),\n    ...(systemMetrics.memory_usage?.percent > 90 ? [{ level: 'critical', message: 'High memory usage detected' }] : []),\n    ...(systemMetrics.disk_usage?.free < 1000000000 ? [{ level: 'warning', message: 'Low disk space' }] : []),\n    ...(Object.values(services).filter(s => s.status !== 'running').length > 0 ? [{ level: 'warning', message: 'Some services are not running' }] : [])\n  ]\n};\n\nreturn [{ json: enhancedReport }];"
      },
      "id": "enhanced-analysis",
      "name": "Enhanced System Analysis",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [860, 350]
    },
    {
      "parameters": {
        "operation": "write",
        "fileName": "./shared/enhanced-system-status.json",
        "fileContent": "={{ JSON.stringify($json, null, 2) }}",
        "options": {
          "encoding": "utf8"
        }
      },
      "id": "save-enhanced-report",
      "name": "Save Enhanced Report",
      "type": "n8n-nodes-base.readWriteFile",
      "typeVersion": 1,
      "position": [1080, 350]
    },
    {
      "parameters": {
        "jsCode": "// Generate Oliver-Friendly Status Report\nconst data = $input.first().json;\n\n// Create a conversational summary for Oliver\nconst oliverReport = {\n  greeting: `Hello! I'm reporting on our AI system status as of ${new Date(data.timestamp).toLocaleString()}.`,\n  \n  system_status: {\n    overall_health: data.health_summary.running_services === data.health_summary.total_services ? 'excellent' : \n                   data.health_summary.running_services >= data.health_summary.total_services * 0.75 ? 'good' : 'needs attention',\n    \n    voice_readiness: data.tts_stt_readiness.overall_ready ? \n      'I\\'m ready for voice interactions! TTS and STT capabilities are fully prepared.' :\n      `Voice capabilities need setup. Current readiness: Hardware ${data.tts_stt_readiness.hardware_score}%, Services ${data.tts_stt_readiness.services_score}%`,\n    \n    brain_status: data.oliver_status.brain_ready ? \n      `My brain is online and ready! Ollama is running with ${data.services.ollama.models} model(s) available.` :\n      'My brain needs attention - Ollama service or models are missing.',\n    \n    memory_status: data.oliver_status.memory_ready ? \n      `My memory systems are operational with ${data.services.qdrant.collections} collection(s) in the vector database.` :\n      'My memory system (Qdrant vector database) needs to be started.',\n    \n    automation_status: data.oliver_status.automation_ready ? \n      `My automation capabilities are active with ${data.services.n8n.workflows} workflow(s) configured.` :\n      'My automation system (n8n) needs attention.'\n  },\n  \n  immediate_actions: data.recommendations.filter(r => r.priority === 'high').map(r => ({\n    action: r.action,\n    category: r.category,\n    command: r.command || 'Manual intervention required',\n    why: r.impact || r.details || 'Critical for system operation'\n  })),\n  \n  capabilities_ready: {\n    text_conversation: data.oliver_status.brain_ready,\n    voice_conversation: data.oliver_status.voice_interface_ready,\n    memory_recall: data.oliver_status.memory_ready,\n    task_automation: data.oliver_status.automation_ready,\n    system_monitoring: true // This workflow itself\n  },\n  \n  next_enhancements: data.recommendations.filter(r => r.priority === 'medium').map(r => r.action),\n  \n  alerts: data.alerts.map(alert => ({\n    level: alert.level,\n    message: alert.message,\n    urgency: alert.level === 'critical' ? 'immediate' : 'monitor'\n  })),\n  \n  performance_metrics: {\n    cpu_usage: `${data.system.cpu_usage}%`,\n    memory_usage: `${data.system.memory_usage_gb}GB used / ${data.system.memory_available_gb}GB available`,\n    disk_space: `${data.system.disk_free_gb}GB free`,\n    gpu_acceleration: data.system.gpu_available ? 'Available' : 'Not Available'\n  },\n  \n  webhook_data: {\n    // Data structure for other workflows to consume\n    ready_for_voice: data.oliver_status.voice_interface_ready,\n    brain_online: data.oliver_status.brain_ready,\n    system_health: data.health_summary.running_services / data.health_summary.total_services,\n    needs_attention: data.recommendations.filter(r => r.priority === 'high').length > 0,\n    last_check: data.timestamp\n  }\n};\n\nreturn [{ json: oliverReport }];"
      },
      "id": "oliver-report",
      "name": "Generate Oliver Report",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1300, 350]
    },
    {
      "parameters": {
        "operation": "write",
        "fileName": "./shared/oliver-system-status.json",
        "fileContent": "={{ JSON.stringify($json, null, 2) }}",
        "options": {
          "encoding": "utf8"
        }
      },
      "id": "save-oliver-report",
      "name": "Save Oliver Report",
      "type": "n8n-nodes-base.readWriteFile",
      "typeVersion": 1,
      "position": [1520, 350]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "critical-alert",
              "leftValue": "={{ $('Enhanced System Analysis').first().json.alerts.filter(a => a.level === 'critical').length }}",
              "rightValue": 0,
              "operator": {
                "type": "number",
                "operation": "gt"
              }
            }
          ]
        },
        "options": {}
      },
      "id": "check-alerts",
      "name": "Critical Alerts?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [1080, 550]
    },
    {
      "parameters": {
        "content": "ðŸš¨ **CRITICAL ALERT - AI System**\\n\\nTimestamp: {{ new Date().toLocaleString() }}\\n\\n**Critical Issues Detected:**\\n{{ $('Enhanced System Analysis').first().json.alerts.filter(a => a.level === 'critical').map(a => `â€¢ ${a.message}`).join('\\n') }}\\n\\n**System Status:**\\nâ€¢ Running Services: {{ $('Enhanced System Analysis').first().json.health_summary.running_services }}/{{ $('Enhanced System Analysis').first().json.health_summary.total_services }}\\nâ€¢ CPU Usage: {{ $('Enhanced System Analysis').first().json.system.cpu_usage }}%\\nâ€¢ Memory Available: {{ $('Enhanced System Analysis').first().json.system.memory_available_gb }}GB\\n\\n**Immediate Actions Required:**\\n{{ $('Enhanced System Analysis').first().json.recommendations.filter(r => r.priority === 'high').map(r => `â€¢ ${r.action}`).join('\\n') }}\\n\\nPlease check the system immediately.",
        "options": {}
      },
      "id": "critical-notification",
      "name": "Send Critical Alert",
      "type": "n8n-nodes-base.noOp",
      "typeVersion": 1,
      "position": [1300, 550]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $('Generate Oliver Report').first().json }}",
        "options": {}
      },
      "id": "webhook-response",
      "name": "API Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [1520, 200]
    }
  ],
  "connections": {
    "Manual Trigger": {
      "main": [
        [
          {
            "node": "Get System Metrics",
            "type": "main",
            "index": 0
          },
          {
            "node": "Check TTS/STT Readiness",
            "type": "main",
            "index": 0
          },
          {
            "node": "Check Container Info",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Scheduled Health Check": {
      "main": [
        [
          {
            "node": "Get System Metrics",
            "type": "main",
            "index": 0
          },
          {
            "node": "Check TTS/STT Readiness",
            "type": "main",
            "index": 0
          },
          {
            "node": "Check Container Info",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Health Check API": {
      "main": [
        [
          {
            "node": "Get System Metrics",
            "type": "main",
            "index": 0
          },
          {
            "node": "Check TTS/STT Readiness",
            "type": "main",
            "index": 0
          },
          {
            "node": "Check Container Info",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get System Metrics": {
      "main": [
        [
          {
            "node": "Check PostgreSQL",
            "type": "main",
            "index": 0
          },
          {
            "node": "Check Ollama Models",
            "type": "main",
            "index": 0
          },
          {
            "node": "Check Qdrant",
            "type": "main",
            "index": 0
          },
          {
            "node": "Check n8n API",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check PostgreSQL": {
      "main": [
        [
          {
            "node": "Enhanced System Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check Ollama Models": {
      "main": [
        [
          {
            "node": "Enhanced System Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check Qdrant": {
      "main": [
        [
          {
            "node": "Enhanced System Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check n8n API": {
      "main": [
        [
          {
            "node": "Enhanced System Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check TTS/STT Readiness": {
      "main": [
        [
          {
            "node": "Enhanced System Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check Container Info": {
      "main": [
        [
          {
            "node": "Enhanced System Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Enhanced System Analysis": {
      "main": [
        [
          {
            "node": "Save Enhanced Report",
            "type": "main",
            "index": 0
          },
          {
            "node": "Critical Alerts?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save Enhanced Report": {
      "main": [
        [
          {
            "node": "Generate Oliver Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Oliver Report": {
      "main": [
        [
          {
            "node": "Save Oliver Report",
            "type": "main",
            "index": 0
          },
          {
            "node": "API Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Critical Alerts?": {
      "main": [
        [
          {
            "node": "Send Critical Alert",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "pinData": {},
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "tags": [],
  "triggerCount": 0,
  "updatedAt": "2025-08-04T02:50:00.000Z",
  "versionId": "enhanced-v1"
}
